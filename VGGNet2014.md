# VGGNet:2014

**Vert Deep COnvolutional Networks For Large-Scale Image Recognition**

**University of Oxford**

#### 摘要：

在本文中我们研究了卷积神经网络的深度在大型数据集上对其精度的影响，我们的主要贡献就是**使用非常小的3×3卷积核来增加网络的深度进而对网络的性能进行研究，基于此方法可以将网络的深度推广到16-19层**，对现有技术可以有显著改进。

#### 论文出发点或背景:

在此之前，在ILSVRC-2013中表现最好的网络模型使用了更小接受窗口大小和更小的步长在第一个卷积层上，另外一个提升是通过在整张图片上多尺度密集地训练和测试网络。而本文的改进主要是基于神经网络的另外一个特点——深度。

#### 论文创新思路：

使用非常小的3×3卷积核来增加网络的深度进而对网络的性能进行研究，基于此方法可以将网络的深度推广到16-19层

为了对比增加深度的公平性，我们所有的网络层配置都使用相同的设计原则

3×3的卷积核可以通过堆叠增加其感受野，两个3×3等同于一个5×5的感受野，三个等同于一个7×7卷积核的感受野。多个小卷积核的好处就是相对于一个大卷积核在其后面增加的多个非线性激活函数可以增强网络的非线性，同时也可以降低参数。

1×1卷积核的使用：增加非线性，1×1卷积核本质上是对输入做了一个线性变换，之后经过了非线性的激活函数，因而增加了网络的非线性。

#### 论文方法的大概介绍：

我们调整了网络中的其他参数，并通过增加卷积层的个数进而增加网络的深度，这也被证明是可行的，因为我们在所有层上都使用的是3×3的小型卷积核。

为了对比增加深度的公平性，我们所有的网络层配置都使用相同的设计原则

预处理：[R,G,B]--->[R-mean_R,G-mean_G,B-mean_B]

所有的隐藏层都带有一个非线性激活函数ReLU

提到LRN并没有提高性能，反而导致了内存消耗和计算时间的增加。

权重随机初始化为零均值和具有$10^{-2}$方差的正态分布，偏置被初始化为0

使用了随机裁剪、随即水平翻转和随机RGB变化.

多尺度裁剪

密集评估：将全连接层改为卷积层

预训练：先训练一个浅层网络，然后用得到的权重去初始化更深层次的网络的层的参数

caffe，4块gpu上分布式训练

#### 实际效果：

虽然之前有人使用过小型卷积核，但是他们的网络深度比我们低，还没有在大型数据集上测试过

![image-20221113100128869](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113100128869.png)

![image-20221113103029161](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113103029161.png)

![image-20221113103057269](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113103057269.png)

![image-20221113103112139](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113103112139.png)

![image-20221113103133978](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113103133978.png)

![image-20221113103145099](C:\Users\李鑫\AppData\Roaming\Typora\typora-user-images\image-20221113103145099.png)

#### 个人理解：

1.实验证明LRN对模型的准确率提升没用

2.网络的初始化十分重要，可以通过预训练方式来防止深层的网络训练失败、

3.网络的深度十分重要，卷积神经网络中，随着特征图空间上信息的减少，通道数会相应增加，将低层次的空间信息转换为了抽象的语义信息

4.对比之前使用的大卷积核和大步长，小卷积核和小步长更能减少空间信息的损失，同时小卷积模块的堆叠可以等效大卷积模块的感受野





